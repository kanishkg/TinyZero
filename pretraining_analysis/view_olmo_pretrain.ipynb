{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anikait.singh/miniconda3/envs/sglang/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'text', 'date', 'metadata', 'backtracking_raw', 'is_solution_raw', 'verification_raw', 'subgoal_setting_raw', 'backward_chaining_raw'],\n",
       "    num_rows: 777000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset('Asap7772/open_web_math_raw_0_1000000', split='train')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_variables(markdown_text):\n",
    "    \"\"\"\n",
    "    Extracts variable values from a markdown string.\n",
    "    The markdown is expected to have sections where a header (starting with '##')\n",
    "    is immediately followed (after optional blank lines) by a single line\n",
    "    containing the variable value.\n",
    "    \n",
    "    Args:\n",
    "        markdown_text (str): The markdown content as a string.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are header texts and values are the extracted variable lines.\n",
    "    \"\"\"\n",
    "    # This regex works as follows:\n",
    "    # - It finds a header line that starts with '##', capturing any text that follows.\n",
    "    # - It then matches one or more newline characters (allowing for blank lines),\n",
    "    #   and captures the first non-empty line that follows as the variable value.\n",
    "    pattern = r\"##\\s*(.*?)\\s*\\n+(?!##)([^\\n]+)\"\n",
    "    \n",
    "    matches = re.findall(pattern, markdown_text)\n",
    "    \n",
    "    variables = {}\n",
    "    for header, var in matches:\n",
    "        # Strip any extra whitespace from the variable text\n",
    "        variables[header] = var.strip()\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.shaalaa.com/question-bank-solutions/the-equations-given-of-the-two-regression-lines-are-2x-3y-6-0-and-5x-7y-12-0-find-a-correlation-coefficient-b-x-y-lines-regression-x-y-y-x-or-equation-line-regression_599',\n",
       " 'text': 'The equations given of the two regression lines are 2x + 3y - 6 = 0 and 5x + 7y - 12 = 0. Find: (a) Correlation coefficient (b) σxσy - Mathematics and Statistics\\n\\nSum\\n\\nThe equations given of the two regression lines are 2x + 3y - 6 = 0 and 5x + 7y - 12 = 0.\\n\\nFind:\\n\\n(a) Correlation coefficient\\n\\n(b)\\xa0sigma_x/sigma_y\\n\\nSolution\\n\\nWe assume that 2x + 3y - 6 = 0 to be the line of regression of y on x.\\n\\n2x + 3y - 6 = 0\\n\\n⇒ x = - 3/2y + 3\\n\\n⇒ \"bxy\" = - 3/2\\n\\n5x + 7y - 12 = 0 to be the line of regression of x on y.\\n\\n5x + 7y - 12 = 0\\n\\n⇒ y = - 5/7x + 12/7\\n\\n⇒\\xa0 \"byx\" = - 5/7\\n\\nNow,\\n\\nr = sqrt(\"bxy.byx\") = sqrt(15/14)\\n\\nbyx = (rσ_y)/(σ_x) = - 5/7, \"bxy\" = (rσ_x)/(σ_y) = - 3/2\\n\\n⇒ (σ_x^2)/(σ_y^2) = \\xa0(3/2)/(5/7)\\n\\n⇒ (σ_x^2)/(σ_y^2) = 21/10\\n\\n⇒ (σ_x)/(σ_y) = sqrt(21/10).\\n\\nConcept: Lines of Regression of X on Y and Y on X Or Equation of Line of Regression\\nIs there an error in this question or solution?',\n",
       " 'date': '2021-11-30 23:57:57',\n",
       " 'metadata': '{\"extraction_info\": {\"found_math\": true, \"script_math_tex\": 0, \"script_math_asciimath\": 0, \"math_annotations\": 0, \"math_alttext\": 0, \"mathml\": 0, \"mathjax_tag\": 0, \"mathjax_inline_tex\": 0, \"mathjax_display_tex\": 0, \"mathjax_asciimath\": 1, \"img_math\": 0, \"codecogs_latex\": 0, \"wp_latex\": 0, \"mimetex.cgi\": 0, \"/images/math/codecogs\": 0, \"mathtex.cgi\": 0, \"katex\": 0, \"math-container\": 0, \"wp-katex-eq\": 0, \"align\": 0, \"equation\": 0, \"x-ck12\": 0, \"texerror\": 0, \"math_score\": 0.425394207239151, \"perplexity\": 1170.7012246572917}, \"config\": {\"markdown_headings\": false, \"markdown_code\": true, \"boilerplate_config\": {\"ratio_threshold\": 0.18, \"absolute_threshold\": 10, \"end_threshold\": 15, \"enable\": true}, \"remove_buttons\": true, \"remove_image_figures\": true, \"remove_link_clusters\": true, \"table_config\": {\"min_rows\": 2, \"min_cols\": 3, \"format\": \"plain\"}, \"remove_chinese\": true, \"remove_edit_buttons\": true, \"extract_latex\": true}, \"warc_path\": \"s3://commoncrawl/crawl-data/CC-MAIN-2021-49/segments/1637964359082.76/warc/CC-MAIN-20211130232232-20211201022232-00063.warc.gz\"}',\n",
       " 'backtracking_raw': \"<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does backtrack?\\nno\\n\\n## Number of backtrack steps\\n<count>0</count>.\\n\\n## Rationale\\nThe provided text does not exhibit any backtracking behavior. The writer presents a problem, outlines the equations given, and proceeds step-by-step to solve for the correlation coefficient and the ratio of standard deviations without indicating any realization that a path won't work or explicitly going back to try a different approach. The solution unfolds in a linear and logical manner, suggesting that the writer did not abandon any thoughts or backtrack to previous computations.\",\n",
       " 'is_solution_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Contains Problem?\\nyes\\n\\n## Contains Solution?\\nyes\\n\\n## Text domain (broad)\\nmath\\n\\n## Text domain (specific)\\nStatistics and Regression Analysis\\n\\n## Rationale\\nThe text provides two equations of regression lines and asks to find the correlation coefficient and the ratio of standard deviations, which indicates a problem in the domain of statistics. The solution provided calculates these values using formulas related to regression analysis, confirming that the text belongs to the specific domain of statistics and regression analysis.',\n",
       " 'verification_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does verification?\\nno\\n\\n## Number of answer verification steps\\n<count>0</count>.\\n\\n## Rationale\\nThe provided text does not contain any explicit verification steps where the writer checks their own work by comparing the result to a known value or by checking the result of a calculation. The solution involves a series of mathematical derivations and calculations to find the correlation coefficient and the ratio of standard deviations, but it does not include any steps that verify the accuracy of these calculations by comparing them to known values or expected results.',\n",
       " 'subgoal_setting_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does answer verification?\\nno\\n\\n## Number of answer verification steps\\n<count>0</count>.\\n\\n## Rationale\\nThe provided text does not contain explicit subgoal setting where the model breaks down the problem into smaller, intermediate goals for the purpose of answer verification. The text presents a mathematical problem and its solution, following a straightforward step-by-step approach to find the correlation coefficient and the ratio of standard deviations. However, it does not explicitly verify the answer through subgoal setting. The steps are more about solving the given mathematical problem rather than verifying an answer through intermediate checks or subgoals.',\n",
       " 'backward_chaining_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does the text exhibit backward chaining?\\nyes\\n\\n## Number of backward chaining instances\\n<count>2</count>\\n\\n## Rationale\\nThe text exhibits backward-chaining behavior in two instances. Firstly, when finding the correlation coefficient (r), the writer starts with the given regression lines and works backward to derive the formula for r, using the slopes \"bxy\" and \"byx\" to ultimately find the value of r. \\n\\nSecondly, when finding σx/σy, the writer starts with the given values of \"byx\" and \"bxy\" and works backward to derive the relationship between σx and σy, using the formulas for \"byx\" and \"bxy\" to ultimately find the ratio σx/σy. In both cases, the writer is working towards a goal (finding r and σx/σy) but starts from the given information (regression lines) and works backward to derive the required values.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted variables:\n",
      "'Does backtrack?': 'no'\n",
      "'Number of backtrack steps': '<count>0</count>.'\n",
      "'Rationale': 'The provided text does not exhibit any backtracking behavior. The writer presents a problem, outlines the equations given, and proceeds step-by-step to solve for the correlation coefficient and the ratio of standard deviations without indicating any realization that a path won't work or explicitly going back to try a different approach. The solution unfolds in a linear and logical manner, suggesting that the writer did not abandon any thoughts or backtrack to previous computations.'\n"
     ]
    }
   ],
   "source": [
    "extracted_vars = extract_variables(ds['backtracking_raw'][0])\n",
    "print(\"Extracted variables:\")\n",
    "for header, var in extracted_vars.items():\n",
    "    print(f\"'{header}': '{var}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=24): 100%|██████████| 777000/777000 [00:07<00:00, 108713.56 examples/s]\n",
      "Map (num_proc=24): 100%|██████████| 777000/777000 [00:11<00:00, 65230.26 examples/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def map_fn_backtrack(examples):\n",
    "    ret_dict = {}\n",
    "    # Process each sample in the batch\n",
    "    for i in range(len(examples['text'])):\n",
    "        curr_backtrack = examples['backtracking_raw'][i]\n",
    "        try:\n",
    "            extracted_vars = extract_variables(curr_backtrack)\n",
    "        except Exception:\n",
    "            extracted_vars = {}  # Use an empty dict if extraction fails\n",
    "        \n",
    "        required_keys = ['Does backtrack?', 'Number of backtrack steps', 'Rationale']\n",
    "        mapped_keys = ['is_backtrack', 'backtrack_count', 'backtrack_rationale']\n",
    "        \n",
    "        # Check if all required keys are present\n",
    "        if all(key in extracted_vars for key in required_keys):\n",
    "            for key, mapped_key in zip(required_keys, mapped_keys):\n",
    "                ret_dict.setdefault(mapped_key, []).append(extracted_vars[key])\n",
    "        else:\n",
    "            # Append default values if extraction is incomplete\n",
    "            for mapped_key in mapped_keys:\n",
    "                ret_dict.setdefault(mapped_key, []).append(None)\n",
    "        \n",
    "        # Always append the original example values\n",
    "        for k in examples.keys():\n",
    "            ret_dict.setdefault(k, []).append(examples[k][i])\n",
    "            \n",
    "    return ret_dict\n",
    "\n",
    "def map_fn_backchain(examples):\n",
    "    ret_dict = {}\n",
    "    for i in range(len(examples['text'])):\n",
    "        curr_backchain = examples['backward_chaining_raw'][i]\n",
    "        try:\n",
    "            extracted_vars = extract_variables(curr_backchain)\n",
    "        except Exception:\n",
    "            extracted_vars = {}\n",
    "        \n",
    "        required_keys = ['Does the text exhibit backward chaining?', 'Number of backward chaining instances', 'Rationale']\n",
    "        mapped_keys = ['is_backchain', 'backchain_count', 'backchain_rationale']\n",
    "        \n",
    "        if all(key in extracted_vars for key in required_keys):\n",
    "            for key, mapped_key in zip(required_keys, mapped_keys):\n",
    "                ret_dict.setdefault(mapped_key, []).append(extracted_vars[key])\n",
    "        else:\n",
    "            for mapped_key in mapped_keys:\n",
    "                ret_dict.setdefault(mapped_key, []).append(None)\n",
    "        \n",
    "        for k in examples.keys():\n",
    "            ret_dict.setdefault(k, []).append(examples[k][i])\n",
    "            \n",
    "    return ret_dict\n",
    "\n",
    "def map_fn_verification(examples):\n",
    "    ret_dict = {}\n",
    "    for i in range(len(examples['text'])):            \n",
    "        curr_verification = examples['verification_raw'][i]\n",
    "        try:\n",
    "            extracted_vars = extract_variables(curr_verification)\n",
    "        except Exception:\n",
    "            extracted_vars = {}\n",
    "        \n",
    "        required_keys = ['Does verification?', 'Number of answer verification steps', 'Rationale']\n",
    "        mapped_keys = ['is_verification', 'verification_count', 'verification_rationale']\n",
    "        \n",
    "        if all(key in extracted_vars for key in required_keys):\n",
    "            for key, mapped_key in zip(required_keys, mapped_keys):\n",
    "                ret_dict.setdefault(mapped_key, []).append(extracted_vars[key])\n",
    "        else:\n",
    "            for mapped_key in mapped_keys:\n",
    "                ret_dict.setdefault(mapped_key, []).append(None)\n",
    "        \n",
    "        for k in examples.keys():\n",
    "            ret_dict.setdefault(k, []).append(examples[k][i])\n",
    "            \n",
    "    return ret_dict\n",
    "\n",
    "def map_fn_solution(examples):\n",
    "    ret_dict = {}\n",
    "    for i in range(len(examples['text'])):            \n",
    "        curr_is_solution = examples['is_solution_raw'][i]\n",
    "        try:\n",
    "            extracted_vars = extract_variables(curr_is_solution)\n",
    "        except Exception:\n",
    "            extracted_vars = {}\n",
    "        \n",
    "        required_keys = ['Contains Problem?', 'Contains Solution?', 'Text domain (broad)', 'Text domain (specific)', 'Rationale']\n",
    "        mapped_keys = ['contain_problem', 'contain_solution', 'domain_broad', 'domain_specific', 'solution_rationale']\n",
    "        \n",
    "        if all(key in extracted_vars for key in required_keys):\n",
    "            for key, mapped_key in zip(required_keys, mapped_keys):\n",
    "                ret_dict.setdefault(mapped_key, []).append(extracted_vars[key])\n",
    "        else:\n",
    "            for mapped_key in mapped_keys:\n",
    "                ret_dict.setdefault(mapped_key, []).append(None)\n",
    "        \n",
    "        for k in examples.keys():\n",
    "            ret_dict.setdefault(k, []).append(examples[k][i])\n",
    "            \n",
    "    return ret_dict\n",
    "\n",
    "# Apply the mapping functions\n",
    "ds = ds.map(map_fn_backtrack, batched=True, remove_columns=ds.column_names, num_proc=os.cpu_count())\n",
    "ds = ds.map(map_fn_backchain, batched=True, remove_columns=ds.column_names, num_proc=os.cpu_count())\n",
    "ds = ds.map(map_fn_verification, batched=True, remove_columns=ds.column_names, num_proc=os.cpu_count())\n",
    "ds = ds.map(map_fn_solution, batched=True, remove_columns=ds.column_names, num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'text', 'date', 'metadata', 'backtracking_raw', 'is_solution_raw', 'verification_raw', 'subgoal_setting_raw', 'backward_chaining_raw', 'is_backtrack', 'backtrack_count', 'backtrack_rationale', 'is_backchain', 'backchain_count', 'backchain_rationale', 'is_verification', 'verification_count', 'verification_rationale', 'contain_problem', 'contain_solution', 'domain_broad', 'domain_specific', 'solution_rationale'],\n",
       "    num_rows: 777000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=24): 100%|██████████| 777000/777000 [00:07<00:00, 106694.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'text', 'date', 'metadata', 'backtracking_raw', 'is_solution_raw', 'verification_raw', 'subgoal_setting_raw', 'backward_chaining_raw', 'is_backtrack', 'backtrack_count', 'backtrack_rationale', 'is_backchain', 'backchain_count', 'backchain_rationale', 'is_verification', 'verification_count', 'verification_rationale', 'contain_problem', 'contain_solution', 'domain_broad', 'domain_specific', 'solution_rationale'],\n",
       "    num_rows: 46467\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_backtrack = ds.filter(lambda x: x['is_backtrack'] and x['is_backtrack'].lower() == 'yes', num_proc=os.cpu_count())\n",
    "filtered_ds_backtrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=24): 100%|██████████| 777000/777000 [00:02<00:00, 304934.20 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'text', 'date', 'metadata', 'backtracking_raw', 'is_solution_raw', 'verification_raw', 'subgoal_setting_raw', 'backward_chaining_raw', 'is_backtrack', 'backtrack_count', 'backtrack_rationale', 'is_backchain', 'backchain_count', 'backchain_rationale', 'is_verification', 'verification_count', 'verification_rationale', 'contain_problem', 'contain_solution', 'domain_broad', 'domain_specific', 'solution_rationale'],\n",
       "    num_rows: 274768\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_backchain = ds.filter(lambda x: x['is_backchain'] and x['is_backchain'].lower() == 'yes', num_proc=os.cpu_count())\n",
    "filtered_ds_backchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=24): 100%|██████████| 777000/777000 [00:02<00:00, 323855.58 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'text', 'date', 'metadata', 'backtracking_raw', 'is_solution_raw', 'verification_raw', 'subgoal_setting_raw', 'backward_chaining_raw', 'is_backtrack', 'backtrack_count', 'backtrack_rationale', 'is_backchain', 'backchain_count', 'backchain_rationale', 'is_verification', 'verification_count', 'verification_rationale', 'contain_problem', 'contain_solution', 'domain_broad', 'domain_specific', 'solution_rationale'],\n",
       "    num_rows: 164377\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_verified = ds.filter(lambda x: x['is_verification'] and x['is_verification'].lower() == 'yes', num_proc=os.cpu_count())\n",
    "filtered_ds_verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://dsp.stackexchange.com/questions/47210/duality-of-the-continuous-time-fourier-transform-derivation-and-notation?noredirect=1',\n",
       " 'text': '# Duality of the continuous-time Fourier transform - derivation and notation\\n\\nSuppose we have the Fourier transform pair $x(t)$ and $X(\\\\omega)$ such that $$X(\\\\omega) = \\\\int_{-\\\\infty}^{\\\\infty} x(t) e^{-j\\\\omega t} \\\\mathrm{d}t$$\\n\\nThe duality property states that $X(t)$ and $2\\\\pi x(-\\\\omega)$ constitute a Fourier transform pair. I was trying to prove this statement when I ran into the following problem: the Fourier transform of a signal is sometimes denoted by $X(j\\\\omega)$, such that $$X(j\\\\omega) = \\\\int_{-\\\\infty}^{\\\\infty} x(t) e^{-j\\\\omega t} \\\\mathrm{d}t$$\\n\\nUsing this notation, how do we even state the duality property? Note that evaluating $X(t)$ in this case would effectively replace $j\\\\omega$ (as opposed to just $\\\\omega$ in the previous case) by $t$ which is clearly wrong. Would we have to write $X(jt)$ for consistency?\\n\\nReturning to the question of deriving the duality property and using the notation I used in the beginning, what is the flaw in the following approach?\\n\\nSince $$X(\\\\omega) = \\\\int_{-\\\\infty}^{\\\\infty} x(t) e^{-j\\\\omega t} \\\\mathrm{d}t$$ we let $t$ be dummy variable $u$, and evaluate the function $X(\\\\cdot)$ at $t$. We then have $$X(t) = \\\\int_{-\\\\infty}^{\\\\infty} x(u) e^{-j ut} \\\\mathrm{d}u$$ With the change of variables $\\\\omega = -u$ we get $$X(t) = -\\\\int_{-\\\\infty}^{\\\\infty} x(-\\\\omega) e^{j\\\\omega t} \\\\mathrm{d}\\\\omega$$ $$X(t) = -\\\\frac{1}{2\\\\pi}\\\\int_{-\\\\infty}^{\\\\infty} 2\\\\pi x(-\\\\omega) e^{j\\\\omega t} \\\\mathrm{d}\\\\omega$$\\n\\nWhich is almost the correct result except for the additional minus sign. Where did I go wrong? Thank you in advance.\\n\\nI would also like to add that I feel uncomfortable about simply changing $t$ and $\\\\omega$ in the functions\\' argument because they have different dimensions.\\n\\nYou\\'ve answered your first question yourself: if you use the notation $X(j\\\\omega)$ then you\\'ll need to use $X(jt)$, because you simply replace $\\\\omega$ by $t$.\\n\\nThe sign error in your derivation comes from the fact that you forgot to change the sign of the limits of the integral. After substituting $\\\\omega=-u$ you should get\\n\\n$$X(t) = -\\\\int_{\\\\infty}^{-\\\\infty} x(-\\\\omega) e^{j\\\\omega t} \\\\mathrm{d}\\\\omega = \\\\int_{-\\\\infty}^{\\\\infty} x(-\\\\omega) e^{j\\\\omega t} \\\\mathrm{d}\\\\omega$$\\n\\n• And using that notation and the duality property, how could we prove the duality of the differentiation property for instance: the Fourier Transform of $-jt x(t)$ is $\\\\frac{dX(j\\\\omega)}{d\\\\omega}$\\n–\\xa0user33568\\nFeb 17 \\'18 at 1:58\\n• Or would this require another post?\\n–\\xa0user33568\\nFeb 17 \\'18 at 2:01\\n• @0MW: I don\\'t think that the fact $X(t)\\\\Longleftrightarrow 2\\\\pi x(-\\\\omega)$ helps a lot in deriving the differentiation property. I think it\\'s easier to derive it directly. Feb 18 \\'18 at 16:07\\n\\nTwo notes about notation and some philosophy of mathematics.\\n\\n1. Notation: Mostly for the utility and elegance regarding the property of Duality, I prefer this consistent definition of the continuous Fourier Transform: $$X(f) \\\\triangleq \\\\mathscr{F} \\\\Big\\\\{ x(t) \\\\Big\\\\} \\\\triangleq \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} x(t) \\\\, e^{-j 2 \\\\pi f t} \\\\ \\\\mathrm{d}t$$ and inverse: $$x(t) \\\\triangleq \\\\mathscr{F}^{-1} \\\\Big\\\\{ X(f) \\\\Big\\\\} = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} X(f) \\\\, e^{+j 2 \\\\pi f t} \\\\ \\\\mathrm{d}f$$ Even with different signs on $j$, the elegant symmetry between the forward transform and inverse should be clear. \\n\\n2. While there is a quantitative difference between $-j$ and $+j$ (they are non-zero and they are negatives of each other), there is no qualitative difference between the two. They are both totally \"imaginary\", they have the same magnitude and, in reflection, they have the same angle. And most saliently, they are imaginary numbers that have equal claim to squaring to be $-1$. With any non-zero number, there are two square roots to $-1$ and both $-j$ and $+j$ have equal claim to being that $\\\\sqrt{-1}$. It\\'s like saying that clockwise and counterclockwise have equal claim to being \"positive\" rotation in mathematics. They are equivalent but opposite. Left and Right are similar. Whether the number line runs from left to right or opposite is just a matter of convention. Now this does not mean that the real numbers $+1$ and $-1$ are equivalent. One is the multiplicative identity and the other is not. But there is absolutely nothing essential you can say about $+j$ that is not also true for $-j$. You could go to every mathematics book and swap $+j$ and $-j$ everywhere and every theorem would be just as valid as it was before.\\n\\nThat said, it should be clear that there is no qualitative difference between the Fourier Transform and the Inverse Fourier Transform as expressed above. But that change in sign will mean a change in sign for either $f$ or $t$ (but not both, take your pick). Now if either $x(t)$ or $X(f)$ have even symmetry, then you need not worry about the change in sign with $t$ or $f$.\\n\\nLike it\\'s pretty easy to show that\\n\\n$$\\\\mathscr{F} \\\\Big\\\\{ \\\\operatorname{rect}(t) \\\\Big\\\\} = \\\\operatorname{sinc}(f)$$\\n\\nwhere\\n\\n$$\\\\operatorname{rect}(u) \\\\triangleq \\\\begin{cases} 1 \\\\qquad & \\\\mathrm{for} \\\\ |u| < \\\\tfrac12 \\\\\\\\ \\\\tfrac12 \\\\qquad & \\\\mathrm{for} \\\\ |u| = \\\\tfrac12 \\\\\\\\ 0 \\\\qquad & \\\\mathrm{for} \\\\ |u| > \\\\tfrac12 \\\\\\\\ \\\\end{cases}$$\\n\\nand\\n\\n$$\\\\operatorname{sinc}(u) \\\\triangleq \\\\begin{cases} 1 \\\\qquad & \\\\mathrm{for} \\\\ u = 0 \\\\\\\\ \\\\frac{\\\\sin(\\\\pi u)}{\\\\pi u} \\\\qquad & \\\\mathrm{for} \\\\ u \\\\ne 0 \\\\\\\\ \\\\end{cases}$$\\n\\nSo, accordingly,\\n\\n$$\\\\mathscr{F}^{-1} \\\\Big\\\\{ \\\\operatorname{sinc}(f) \\\\Big\\\\} = \\\\operatorname{rect}(t)$$\\n\\nGee, I wonder what $\\\\mathscr{F} \\\\Big\\\\{ \\\\operatorname{sinc}(t) \\\\Big\\\\}$ is?\\n\\nAs Matt pointed out, you did miss swapping the integral limits, but I would recommend changing your definitions to the above and your life with Duality will get a whole lot easier:\\n\\nIf $X(f) = \\\\mathscr{F} \\\\Big\\\\{ x(t) \\\\Big\\\\}$, then $x(-f) = \\\\mathscr{F} \\\\Big\\\\{ X(t) \\\\Big\\\\}$.\\n\\nYour life with $X(0)$ and $x(0)$ and with Parseval\\'s Theorem will get easier, too:\\n\\n$$X(0) = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} x(t) \\\\ \\\\mathrm{d}t$$\\n\\n$$x(0) = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} X(f) \\\\ \\\\mathrm{d}f$$\\n\\n$$\\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} \\\\Big| x(t) \\\\Big|^2 \\\\ \\\\mathrm{d}t = \\\\int\\\\limits_{-\\\\infty}^{+\\\\infty} \\\\Big| X(f) \\\\Big|^2 \\\\ \\\\mathrm{d}f$$\\n\\nNO nasty asymmetrical scaling factors to worry about!! (Just remember the $2\\\\pi$ in the exponent.)\\n\\nAnd you will be able to answer the question of how many functions of $t$ are exactly the same as their Fourier transform (with $f$ substituted for $t$). It\\'s not just the Gaussian:\\n\\n$$x(t) = e^{-j \\\\pi t^2}$$\\n\\nwhich happens to have an identical Fourier Transform:\\n\\n$$X(f) = e^{-j \\\\pi f^2}$$\\n\\nJust everything in your continuous F.T. life will be easier if you use this notation.',\n",
       " 'date': '2021-09-23 18:11:41',\n",
       " 'metadata': '{\"extraction_info\": {\"found_math\": true, \"script_math_tex\": 0, \"script_math_asciimath\": 0, \"math_annotations\": 0, \"math_alttext\": 0, \"mathml\": 0, \"mathjax_tag\": 0, \"mathjax_inline_tex\": 1, \"mathjax_display_tex\": 1, \"mathjax_asciimath\": 0, \"img_math\": 0, \"codecogs_latex\": 0, \"wp_latex\": 0, \"mimetex.cgi\": 0, \"/images/math/codecogs\": 0, \"mathtex.cgi\": 0, \"katex\": 0, \"math-container\": 0, \"wp-katex-eq\": 0, \"align\": 0, \"equation\": 0, \"x-ck12\": 0, \"texerror\": 0, \"math_score\": 0.9124709367752075, \"perplexity\": 153.49062792874184}, \"config\": {\"markdown_headings\": true, \"markdown_code\": true, \"boilerplate_config\": {\"ratio_threshold\": 0.18, \"absolute_threshold\": 10, \"end_threshold\": 15, \"enable\": true}, \"remove_buttons\": true, \"remove_image_figures\": true, \"remove_link_clusters\": true, \"table_config\": {\"min_rows\": 2, \"min_cols\": 3, \"format\": \"plain\"}, \"remove_chinese\": true, \"remove_edit_buttons\": true, \"extract_latex\": true}, \"warc_path\": \"s3://commoncrawl/crawl-data/CC-MAIN-2021-39/segments/1631780057427.71/warc/CC-MAIN-20210923165408-20210923195408-00280.warc.gz\"}',\n",
       " 'backtracking_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does backtrack?\\nyes\\n\\n## Number of backtrack steps\\n<count> 2 </count>.\\n\\n## Rationale\\nThe writer backtracks twice in the text. The first instance occurs when the writer is trying to prove the duality property and realizes that the notation $X(j\\\\omega)$ would require $X(jt)$ for consistency, but then decides to return to the original notation to derive the property. The second instance occurs when the writer attempts to derive the duality property using a specific approach, but then realizes a flaw in the approach (a missing sign change in the integral limits) and backtracks to correct the mistake.',\n",
       " 'is_solution_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Contains Problem?\\nyes\\n\\n## Contains Solution?\\nyes\\n\\n## Text domain (broad)\\nmath/science\\n\\n## Text domain (specific)\\nSignal Processing, specifically Fourier Transforms\\n\\n## Rationale\\nThe text discusses the duality property of the continuous-time Fourier transform, which is a concept in signal processing. The author encounters a problem with notation and derivation, and the solution is provided in the text, explaining the correct approach to prove the duality property and addressing the issue with the notation. The text also touches on related topics, such as the differentiation property of the Fourier transform.',\n",
       " 'verification_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does verification?\\nyes\\n\\n## Number of answer verification steps\\n<count> 2 </count>.\\n\\n## Rationale\\nThe writer explicitly checks their own work in two instances. Firstly, they attempt to derive the duality property of the Fourier transform and compare their result to the known correct result, identifying a sign error in their derivation. Secondly, they discuss the consistency of notation and how it affects the derivation of properties such as the duality of the differentiation property, implying a verification of their understanding of the notation and its implications.',\n",
       " 'subgoal_setting_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does answer verification?\\nno\\n\\n## Number of answer verification steps\\n<count>0</count>.\\n\\n## Rationale\\nThe provided text does not contain explicit subgoal setting where the model breaks down the problem into smaller, intermediate goals. The text appears to be a discussion about the duality property of the continuous-time Fourier transform, with the author attempting to derive the property and seeking clarification on notation. The response provided is also a discussion on notation and the properties of the Fourier transform, but it does not involve breaking down a problem into smaller subgoals.',\n",
       " 'backward_chaining_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does the text exhibit backward chaining?\\nyes\\n\\n## Number of backward chaining instances\\n<count>2</count>\\n\\n## Rationale\\nThe text exhibits backward-chaining behavior in two instances. The first instance is when the writer is trying to derive the duality property of the Fourier transform and starts from the goal, which is to prove that $X(t)$ and $2\\\\pi x(-\\\\omega)$ constitute a Fourier transform pair. The writer then works backward to find the flaw in their approach.\\n\\nThe second instance is when the writer is trying to prove the duality of the differentiation property, which is the Fourier Transform of $-jt x(t)$ being $\\\\frac{dX(j\\\\omega)}{d\\\\omega}$. Although the writer does not fully derive this property, they start from the goal and discuss how to approach it, which is another example of backward chaining.',\n",
       " 'is_backtrack': 'yes',\n",
       " 'backtrack_count': '<count> 2 </count>.',\n",
       " 'backtrack_rationale': 'The writer backtracks twice in the text. The first instance occurs when the writer is trying to prove the duality property and realizes that the notation $X(j\\\\omega)$ would require $X(jt)$ for consistency, but then decides to return to the original notation to derive the property. The second instance occurs when the writer attempts to derive the duality property using a specific approach, but then realizes a flaw in the approach (a missing sign change in the integral limits) and backtracks to correct the mistake.',\n",
       " 'is_backchain': 'yes',\n",
       " 'backchain_count': '<count>2</count>',\n",
       " 'backchain_rationale': 'The text exhibits backward-chaining behavior in two instances. The first instance is when the writer is trying to derive the duality property of the Fourier transform and starts from the goal, which is to prove that $X(t)$ and $2\\\\pi x(-\\\\omega)$ constitute a Fourier transform pair. The writer then works backward to find the flaw in their approach.',\n",
       " 'is_verification': 'yes',\n",
       " 'verification_count': '<count> 2 </count>.',\n",
       " 'verification_rationale': 'The writer explicitly checks their own work in two instances. Firstly, they attempt to derive the duality property of the Fourier transform and compare their result to the known correct result, identifying a sign error in their derivation. Secondly, they discuss the consistency of notation and how it affects the derivation of properties such as the duality of the differentiation property, implying a verification of their understanding of the notation and its implications.',\n",
       " 'contain_problem': 'yes',\n",
       " 'contain_solution': 'yes',\n",
       " 'domain_broad': 'math/science',\n",
       " 'domain_specific': 'Signal Processing, specifically Fourier Transforms',\n",
       " 'solution_rationale': 'The text discusses the duality property of the continuous-time Fourier transform, which is a concept in signal processing. The author encounters a problem with notation and derivation, and the solution is provided in the text, explaining the correct approach to prove the duality property and addressing the issue with the notation. The text also touches on related topics, such as the differentiation property of the Fourier transform.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_backtrack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Duality of the continuous-time Fourier transform - derivation and notation\n",
      "\n",
      "Suppose we have the Fourier transform pair $x(t)$ and $X(\\omega)$ such that $$X(\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} \\mathrm{d}t$$\n",
      "\n",
      "The duality property states that $X(t)$ and $2\\pi x(-\\omega)$ constitute a Fourier transform pair. I was trying to prove this statement when I ran into the following problem: the Fourier transform of a signal is sometimes denoted by $X(j\\omega)$, such that $$X(j\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} \\mathrm{d}t$$\n",
      "\n",
      "Using this notation, how do we even state the duality property? Note that evaluating $X(t)$ in this case would effectively replace $j\\omega$ (as opposed to just $\\omega$ in the previous case) by $t$ which is clearly wrong. Would we have to write $X(jt)$ for consistency?\n",
      "\n",
      "Returning to the question of deriving the duality property and using the notation I used in the beginning, what is the flaw in the following approach?\n",
      "\n",
      "Since $$X(\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} \\mathrm{d}t$$ we let $t$ be dummy variable $u$, and evaluate the function $X(\\cdot)$ at $t$. We then have $$X(t) = \\int_{-\\infty}^{\\infty} x(u) e^{-j ut} \\mathrm{d}u$$ With the change of variables $\\omega = -u$ we get $$X(t) = -\\int_{-\\infty}^{\\infty} x(-\\omega) e^{j\\omega t} \\mathrm{d}\\omega$$ $$X(t) = -\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} 2\\pi x(-\\omega) e^{j\\omega t} \\mathrm{d}\\omega$$\n",
      "\n",
      "Which is almost the correct result except for the additional minus sign. Where did I go wrong? Thank you in advance.\n",
      "\n",
      "I would also like to add that I feel uncomfortable about simply changing $t$ and $\\omega$ in the functions' argument because they have different dimensions.\n",
      "\n",
      "You've answered your first question yourself: if you use the notation $X(j\\omega)$ then you'll need to use $X(jt)$, because you simply replace $\\omega$ by $t$.\n",
      "\n",
      "The sign error in your derivation comes from the fact that you forgot to change the sign of the limits of the integral. After substituting $\\omega=-u$ you should get\n",
      "\n",
      "$$X(t) = -\\int_{\\infty}^{-\\infty} x(-\\omega) e^{j\\omega t} \\mathrm{d}\\omega = \\int_{-\\infty}^{\\infty} x(-\\omega) e^{j\\omega t} \\mathrm{d}\\omega$$\n",
      "\n",
      "• And using that notation and the duality property, how could we prove the duality of the differentiation property for instance: the Fourier Transform of $-jt x(t)$ is $\\frac{dX(j\\omega)}{d\\omega}$\n",
      "– user33568\n",
      "Feb 17 '18 at 1:58\n",
      "• Or would this require another post?\n",
      "– user33568\n",
      "Feb 17 '18 at 2:01\n",
      "• @0MW: I don't think that the fact $X(t)\\Longleftrightarrow 2\\pi x(-\\omega)$ helps a lot in deriving the differentiation property. I think it's easier to derive it directly. Feb 18 '18 at 16:07\n",
      "\n",
      "Two notes about notation and some philosophy of mathematics.\n",
      "\n",
      "1. Notation: Mostly for the utility and elegance regarding the property of Duality, I prefer this consistent definition of the continuous Fourier Transform: $$X(f) \\triangleq \\mathscr{F} \\Big\\{ x(t) \\Big\\} \\triangleq \\int\\limits_{-\\infty}^{+\\infty} x(t) \\, e^{-j 2 \\pi f t} \\ \\mathrm{d}t$$ and inverse: $$x(t) \\triangleq \\mathscr{F}^{-1} \\Big\\{ X(f) \\Big\\} = \\int\\limits_{-\\infty}^{+\\infty} X(f) \\, e^{+j 2 \\pi f t} \\ \\mathrm{d}f$$ Even with different signs on $j$, the elegant symmetry between the forward transform and inverse should be clear. \n",
      "\n",
      "2. While there is a quantitative difference between $-j$ and $+j$ (they are non-zero and they are negatives of each other), there is no qualitative difference between the two. They are both totally \"imaginary\", they have the same magnitude and, in reflection, they have the same angle. And most saliently, they are imaginary numbers that have equal claim to squaring to be $-1$. With any non-zero number, there are two square roots to $-1$ and both $-j$ and $+j$ have equal claim to being that $\\sqrt{-1}$. It's like saying that clockwise and counterclockwise have equal claim to being \"positive\" rotation in mathematics. They are equivalent but opposite. Left and Right are similar. Whether the number line runs from left to right or opposite is just a matter of convention. Now this does not mean that the real numbers $+1$ and $-1$ are equivalent. One is the multiplicative identity and the other is not. But there is absolutely nothing essential you can say about $+j$ that is not also true for $-j$. You could go to every mathematics book and swap $+j$ and $-j$ everywhere and every theorem would be just as valid as it was before.\n",
      "\n",
      "That said, it should be clear that there is no qualitative difference between the Fourier Transform and the Inverse Fourier Transform as expressed above. But that change in sign will mean a change in sign for either $f$ or $t$ (but not both, take your pick). Now if either $x(t)$ or $X(f)$ have even symmetry, then you need not worry about the change in sign with $t$ or $f$.\n",
      "\n",
      "Like it's pretty easy to show that\n",
      "\n",
      "$$\\mathscr{F} \\Big\\{ \\operatorname{rect}(t) \\Big\\} = \\operatorname{sinc}(f)$$\n",
      "\n",
      "where\n",
      "\n",
      "$$\\operatorname{rect}(u) \\triangleq \\begin{cases} 1 \\qquad & \\mathrm{for} \\ |u| < \\tfrac12 \\\\ \\tfrac12 \\qquad & \\mathrm{for} \\ |u| = \\tfrac12 \\\\ 0 \\qquad & \\mathrm{for} \\ |u| > \\tfrac12 \\\\ \\end{cases}$$\n",
      "\n",
      "and\n",
      "\n",
      "$$\\operatorname{sinc}(u) \\triangleq \\begin{cases} 1 \\qquad & \\mathrm{for} \\ u = 0 \\\\ \\frac{\\sin(\\pi u)}{\\pi u} \\qquad & \\mathrm{for} \\ u \\ne 0 \\\\ \\end{cases}$$\n",
      "\n",
      "So, accordingly,\n",
      "\n",
      "$$\\mathscr{F}^{-1} \\Big\\{ \\operatorname{sinc}(f) \\Big\\} = \\operatorname{rect}(t)$$\n",
      "\n",
      "Gee, I wonder what $\\mathscr{F} \\Big\\{ \\operatorname{sinc}(t) \\Big\\}$ is?\n",
      "\n",
      "As Matt pointed out, you did miss swapping the integral limits, but I would recommend changing your definitions to the above and your life with Duality will get a whole lot easier:\n",
      "\n",
      "If $X(f) = \\mathscr{F} \\Big\\{ x(t) \\Big\\}$, then $x(-f) = \\mathscr{F} \\Big\\{ X(t) \\Big\\}$.\n",
      "\n",
      "Your life with $X(0)$ and $x(0)$ and with Parseval's Theorem will get easier, too:\n",
      "\n",
      "$$X(0) = \\int\\limits_{-\\infty}^{+\\infty} x(t) \\ \\mathrm{d}t$$\n",
      "\n",
      "$$x(0) = \\int\\limits_{-\\infty}^{+\\infty} X(f) \\ \\mathrm{d}f$$\n",
      "\n",
      "$$\\int\\limits_{-\\infty}^{+\\infty} \\Big| x(t) \\Big|^2 \\ \\mathrm{d}t = \\int\\limits_{-\\infty}^{+\\infty} \\Big| X(f) \\Big|^2 \\ \\mathrm{d}f$$\n",
      "\n",
      "NO nasty asymmetrical scaling factors to worry about!! (Just remember the $2\\pi$ in the exponent.)\n",
      "\n",
      "And you will be able to answer the question of how many functions of $t$ are exactly the same as their Fourier transform (with $f$ substituted for $t$). It's not just the Gaussian:\n",
      "\n",
      "$$x(t) = e^{-j \\pi t^2}$$\n",
      "\n",
      "which happens to have an identical Fourier Transform:\n",
      "\n",
      "$$X(f) = e^{-j \\pi f^2}$$\n",
      "\n",
      "Just everything in your continuous F.T. life will be easier if you use this notation.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_ds_backtrack[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://pypi.org/project/refactor/0.5.0/',\n",
       " 'text': 'AST-based fragmental source code refactoring toolkit\\n\\n# Refactor\\n\\nSimple, hassle-free, dependency-free, AST based fragmental source code refactoring and transformation toolkit.\\n\\n## Why?\\n\\nOur framework is primarily built on the principle of \"simple but effective transformations\". We focus on refactorings that target a small span of source code, and work our way out from it. What this enables for us is being able to operate directly on a single format for both analyses and transformations. This is what we shine at compared to other similar tools.\\n\\n## How?\\n\\nLet\\'s not get into too much details, but just to give a sneak peek we can try to write a rule that would replace the identifier placeholder with 42.\\n\\nimport ast\\nfrom refactor import Rule, Replace, run\\n\\n# Each refactor transformer inherits from \"refactor.Rule\"\\nclass FillPlaceholders(Rule):\\n\\n# And each rule implements a \"match()\" method, which would\\n# receive every node in the tree in a breadth-first order.\\ndef match(self, node: ast.AST) -> Replace:\\n# This is where things get interesting. Instead of just writing\\n# filters with if statements, you can use the following assert\\n# based approach (a contract of transformation).\\n\\n# For this case, our contract is going to be: if the given node\\n# is an identifier with the name of \"placeholder\", it will be\\n# replaced with literal \"42\".\\nassert isinstance(node, ast.Name)\\nassert node.id == \"placeholder\"\\n\\n# And this is where we choose what action we are taking for the\\n# given node (which we have verified with our contract). There\\n# are multiple transformation actions, but in this case what we\\n# need is something that replaces a node with another one.\\nreplacement = ast.Constant(42)\\nreturn Replace(node, replacement)\\n\\nif __name__ == \"__main__\":\\n# And finally in here, we just use the default CLI that comes\\n# bundled with refactor. When provided with a bunch of rules,\\n# it creates a simple interface that can process given files\\n# show the diff for changes and even apply them.\\nrun(rules=[FillPlaceholders])\\n\\n\\nIf we run the rule above on a file, we can see how it performs:\\n\\n--- test_file.py\\n+++ test_file.py\\n\\n@@ -1,11 +1,11 @@\\n\\ndef main():\\n-    print(placeholder * 3 + 2)\\n+    print(42 * 3 + 2)\\n-    print(2 +               placeholder      + 3)\\n+    print(2 +               42      + 3)\\n-    placeholder # maybe other comments\\n+    42 # maybe other comments\\nif something:\\nother_thing\\n-    print(placeholder)\\n+    print(42)\\n\\nif __name__ == \"__main__\":\\nmain()\\n\\n\\nFor learning more, check our documentation out!\\n\\n## Release history Release notifications | RSS feed\\n\\nUploaded source\\nUploaded py3',\n",
       " 'date': '2022-10-04 10:49:34',\n",
       " 'metadata': '{\"extraction_info\": {\"found_math\": true, \"script_math_tex\": 0, \"script_math_asciimath\": 0, \"math_annotations\": 0, \"math_alttext\": 0, \"mathml\": 0, \"mathjax_tag\": 0, \"mathjax_inline_tex\": 0, \"mathjax_display_tex\": 0, \"mathjax_asciimath\": 1, \"img_math\": 0, \"codecogs_latex\": 0, \"wp_latex\": 0, \"mimetex.cgi\": 0, \"/images/math/codecogs\": 0, \"mathtex.cgi\": 0, \"katex\": 0, \"math-container\": 0, \"wp-katex-eq\": 0, \"align\": 0, \"equation\": 0, \"x-ck12\": 0, \"texerror\": 0, \"math_score\": 0.27125832438468933, \"perplexity\": 6287.603310794307}, \"config\": {\"markdown_headings\": true, \"markdown_code\": true, \"boilerplate_config\": {\"ratio_threshold\": 0.3, \"absolute_threshold\": 10, \"end_threshold\": 15, \"enable\": true}, \"remove_buttons\": true, \"remove_image_figures\": true, \"remove_link_clusters\": true, \"table_config\": {\"min_rows\": 2, \"min_cols\": 3, \"format\": \"plain\"}, \"remove_chinese\": true, \"remove_edit_buttons\": true, \"extract_latex\": true}, \"warc_path\": \"s3://commoncrawl/crawl-data/CC-MAIN-2022-40/segments/1664030337490.6/warc/CC-MAIN-20221004085909-20221004115909-00230.warc.gz\"}',\n",
       " 'backtracking_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does backtrack?\\nno\\n\\n## Number of backtrack steps\\n<count>0</count>.\\n\\n## Rationale\\nThe provided text does not exhibit any backtracking behavior. The writer presents a clear and linear explanation of the AST-based fragmental source code refactoring toolkit, its principles, and an example of how to use it. There are no instances where the writer abandons a thought or approach and explicitly goes back to try a different one, using phrases like \"Let me try again\" or \"Wait\". The text is a straightforward presentation of the toolkit and its usage, without any evidence of backtracking.',\n",
       " 'is_solution_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Contains Problem?\\nno\\n\\n## Contains Solution?\\nyes\\n\\n## Text domain (broad)\\ncomputer science\\n\\n## Text domain (specific)\\nSoftware Engineering, Code Refactoring\\n\\n## Rationale\\nThe text does not explicitly state a problem, but rather presents a solution to a common issue in software development, which is code refactoring. The solution is an AST-based toolkit that allows for simple and effective transformations of source code. The text provides an example of how the toolkit can be used to replace an identifier with a literal value, and includes code snippets and explanations to demonstrate its functionality. The tone is informative and promotional, suggesting that the text is intended to introduce and showcase the toolkit rather than identify a specific problem to be solved.',\n",
       " 'verification_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does verification?\\nyes\\n\\n## Number of answer verification steps\\n<count>2</count>.\\n\\n## Rationale\\nThe writer explicitly checks their own work in two instances. Firstly, within the `match` method of the `FillPlaceholders` class, the writer uses `assert` statements to verify that the given node is an identifier with the name \"placeholder\" before replacing it with the literal \"42\". This is an example of verification as the writer is checking the result of a condition (the node type and id) against a known value. Secondly, the writer provides a diff output after running the rule on a test file, which can be seen as a verification step as it shows the expected changes made by the rule, allowing the writer (and the reader) to verify that the rule performed as intended.',\n",
       " 'subgoal_setting_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does answer verification?\\nno\\n\\n## Number of answer verification steps\\n<count>0</count>.\\n\\n## Rationale\\nThe provided text does not contain explicit subgoal setting where the problem is broken down into smaller, intermediate goals. The text appears to be a documentation or tutorial for an AST-based source code refactoring toolkit, explaining its principles, usage, and providing an example. It does not outline a series of subgoals to achieve a specific objective.',\n",
       " 'backward_chaining_raw': '<|start_header_id|>assistant<|end_header_id|>\\n\\n## Does the text exhibit backward chaining?\\nyes\\n\\n## Number of backward chaining instances\\n<count>2</count>\\n\\n## Rationale\\nThe text exhibits backward-chaining behavior in two instances. \\n\\nFirstly, in the \"How?\" section, the writer starts with the goal of replacing an identifier placeholder with the value 42 and then works backward to explain how this can be achieved using the `refactor` toolkit. They begin with the desired outcome (replacing the placeholder with 42) and then describe the steps needed to implement this transformation, such as defining a rule, implementing a `match()` method, and using an `assert`-based approach to specify the transformation contract.\\n\\nSecondly, in the example code, the writer starts with the goal of refactoring a piece of code to replace all occurrences of \"placeholder\" with 42. They then work backward to explain how this can be achieved using the `FillPlaceholders` rule, which implements the necessary logic to match and replace the placeholder nodes in the abstract syntax tree (AST). The writer presents the desired outcome (the refactored code) and then explains the steps needed to achieve it, demonstrating backward-chaining behavior.',\n",
       " 'is_backtrack': 'no',\n",
       " 'backtrack_count': '<count>0</count>.',\n",
       " 'backtrack_rationale': 'The provided text does not exhibit any backtracking behavior. The writer presents a clear and linear explanation of the AST-based fragmental source code refactoring toolkit, its principles, and an example of how to use it. There are no instances where the writer abandons a thought or approach and explicitly goes back to try a different one, using phrases like \"Let me try again\" or \"Wait\". The text is a straightforward presentation of the toolkit and its usage, without any evidence of backtracking.',\n",
       " 'is_backchain': 'yes',\n",
       " 'backchain_count': '<count>2</count>',\n",
       " 'backchain_rationale': 'The text exhibits backward-chaining behavior in two instances.',\n",
       " 'is_verification': 'yes',\n",
       " 'verification_count': '<count>2</count>.',\n",
       " 'verification_rationale': 'The writer explicitly checks their own work in two instances. Firstly, within the `match` method of the `FillPlaceholders` class, the writer uses `assert` statements to verify that the given node is an identifier with the name \"placeholder\" before replacing it with the literal \"42\". This is an example of verification as the writer is checking the result of a condition (the node type and id) against a known value. Secondly, the writer provides a diff output after running the rule on a test file, which can be seen as a verification step as it shows the expected changes made by the rule, allowing the writer (and the reader) to verify that the rule performed as intended.',\n",
       " 'contain_problem': 'no',\n",
       " 'contain_solution': 'yes',\n",
       " 'domain_broad': 'computer science',\n",
       " 'domain_specific': 'Software Engineering, Code Refactoring',\n",
       " 'solution_rationale': 'The text does not explicitly state a problem, but rather presents a solution to a common issue in software development, which is code refactoring. The solution is an AST-based toolkit that allows for simple and effective transformations of source code. The text provides an example of how the toolkit can be used to replace an identifier with a literal value, and includes code snippets and explanations to demonstrate its functionality. The tone is informative and promotional, suggesting that the text is intended to introduce and showcase the toolkit rather than identify a specific problem to be solved.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_verified[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# multifi_cokriging.py¶\n",
      "\n",
      "Integrates the Multi-Fidelity Co-Kriging method described in [LeGratiet2013].\n",
      "\n",
      "(Author: Remi Vauclin vauclin.remi@gmail.com)\n",
      "\n",
      "This code was implemented using the package scikit-learn as basis. (Author: Vincent Dubourg, vincent.dubourg@gmail.com)\n",
      "\n",
      "OpenMDAO adaptation. Regression and correlation functions were directly copied from scikit-learn package here to avoid scikit-learn dependency. (Author: Remi Lafage, remi.lafage@onera.fr)\n",
      "\n",
      "ISAE/DMSM - ONERA/DCPS\n",
      "\n",
      "class openmdao.surrogate_models.multifi_cokriging.FloatMultiFiCoKrigingSurrogate(regr='constant', rho_regr='constant', theta=None, theta0=None, thetaL=None, thetaU=None, tolerance=1e-06, initial_range=0.3)[source]\n",
      "\n",
      "Predictions are returned as the mean of the NormalDistribution predicted by base class model.\n",
      "\n",
      "__init__(regr='constant', rho_regr='constant', theta=None, theta0=None, thetaL=None, thetaU=None, tolerance=1e-06, initial_range=0.3)\n",
      "\n",
      "Initialize all attributes.\n",
      "\n",
      "Parameters: regr : string or callable, optional A regression function returning an array of outputs of the linear regression functional basis for Universal Kriging purpose. regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ rho_regr : string or callable, optional A regression function returning an array of outputs of the linear regression functional basis. Defines the regression function for the autoregressive parameter rho. rho_regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ theta : double, array_like or list, optional Value of correlation parameters if they are known; no optimization is run. Default is None, so that optimization is run. if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level theta0 : double, array_like or list, optional Starting point for the maximum likelihood estimation of the best set of parameters. Default is None and meaning use of the default 0.5*np.ones(n_features) if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level thetaL : double, array_like or list, optional Lower bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of the default 1e-5*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level thetaU : double, array_like or list, optional Upper bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of default value 50*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level tolerance : float Optimizer terminates when the tolerance tol is reached. initial_range : float Initial range for the optimizer.\n",
      "linearize(x)\n",
      "\n",
      "Calculate the jacobian of the interpolant at the requested point.\n",
      "\n",
      "Parameters: x : array-like Point at which the surrogate Jacobian is evaluated.\n",
      "predict(new_x)[source]\n",
      "\n",
      "Calculate a predicted value of the response based on the current trained model.\n",
      "\n",
      "Parameters: new_x : array_like An array with shape (n_eval, n_features) giving the point(s) at which the prediction(s) should be made. float New predicted value.\n",
      "train(x, y)\n",
      "\n",
      "Calculate a predicted value of the response based on the current trained model.\n",
      "\n",
      "Parameters: x : array-like Point(s) at which the surrogate is evaluated. y : array-like Model responses at given inputs.\n",
      "train_multifi(X, Y)\n",
      "\n",
      "Train the surrogate model with the given set of inputs and outputs.\n",
      "\n",
      "Parameters: X : array_like An array with shape (n_samples_X, n_features) with the input at which observations were made. Y : array_like An array with shape (n_samples_X, n_features) with the observations of the scalar output to be predicted.\n",
      "vectorized_predict(x)\n",
      "\n",
      "Calculate predicted values of the response based on the current trained model.\n",
      "\n",
      "Parameters: x : array-like Vectorized point(s) at which the surrogate is evaluated.\n",
      "class openmdao.surrogate_models.multifi_cokriging.MultiFiCoKriging(regr='constant', rho_regr='constant', theta=None, theta0=None, thetaL=None, thetaU=None)[source]\n",
      "\n",
      "Bases: object\n",
      "\n",
      "Integrate the Multi-Fidelity Co-Kriging method described in [LeGratiet2013].\n",
      "\n",
      "Notes\n",
      "\n",
      "Implementation is based on the Package Scikit-Learn (Author: Vincent Dubourg, vincent.dubourg@gmail.com) which translates the DACE Matlab toolbox, see [NLNS2002].\n",
      "\n",
      "References\n",
      "\n",
      " [NLNS2002] (1, 2) H. B. Nielsen, S. N. Lophaven, and J. Sondergaard. DACE - A MATLAB Kriging Toolbox. (2002) http://www2.imm.dtu.dk/~hbn/dace/dace.pdf\n",
      " [WBSWM1992] W. J. Welch, R. J. Buck, J. Sacks, H. P. Wynn, T. J. Mitchell, and M. D. Morris (1992). “Screening, predicting, and computer experiments.” Technometrics, 34(1) 15–25. http://www.jstor.org/pss/1269548\n",
      " [LeGratiet2013] L. Le Gratiet (2013). “Multi-fidelity Gaussian process regression for computer experiments.” PhD thesis, Universite Paris-Diderot-Paris VII.\n",
      " [TBKH2011] Toal, D. J., Bressloff, N. W., Keane, A. J., & Holden, C. M. E. (2011). “The development of a hybridized particle swarm for kriging hyperparameter tuning.” Engineering optimization, 43(6), 675-699.\n",
      "\n",
      "Examples\n",
      "\n",
      ">>> from openmdao.surrogate_models.multifi_cokriging import MultiFiCoKriging\n",
      ">>> import numpy as np\n",
      ">>> # Xe: DOE for expensive code (nested in Xc)\n",
      ">>> # Xc: DOE for cheap code\n",
      ">>> # ye: expensive response\n",
      ">>> # yc: cheap response\n",
      ">>> Xe = np.array([[0],[0.4],[1]])\n",
      ">>> Xc = np.vstack((np.array([[0.1],[0.2],[0.3],[0.5],[0.6],[0.7],[0.8],[0.9]]),Xe))\n",
      ">>> ye = ((Xe*6-2)**2)*np.sin((Xe*6-2)*2)\n",
      ">>> yc = 0.5*((Xc*6-2)**2)*np.sin((Xc*6-2)*2)+(Xc-0.5)*10. - 5\n",
      ">>> model = MultiFiCoKriging(theta0=1, thetaL=1e-5, thetaU=50.)\n",
      ">>> model.fit([Xc, Xe], [yc, ye])\n",
      ">>> # Prediction on x=0.05\n",
      ">>> np.abs(float(model.predict([0.05])[0])- ((0.05*6-2)**2)*np.sin((0.05*6-2)*2)) < 0.05\n",
      "True\n",
      "\n",
      "\n",
      "Attributes\n",
      "\n",
      " corr (Object) Correlation function to use, default is squared_exponential_correlation. regr (string or callable) A regression function returning an array of outputs of the linear regression functional basis for Universal Kriging purpose. regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ rho_regr (string or callable or None) A regression function returning an array of outputs of the linear regression functional basis. Defines the regression function for the autoregressive parameter rho. rho_regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ theta (double, array_like or list or None) Value of correlation parameters if they are known; no optimization is run. Default is None, so that optimization is run. if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level theta0 (double, array_like or list or None) Starting point for the maximum likelihood estimation of the best set of parameters. Default is None and meaning use of the default 0.5*np.ones(n_features) if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level thetaL (double, array_like or list or None) Lower bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of the default 1e-5*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level thetaU (double, array_like or list or None) Upper bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of default value 50*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level\n",
      "__init__(regr='constant', rho_regr='constant', theta=None, theta0=None, thetaL=None, thetaU=None)[source]\n",
      "\n",
      "Initialize all attributes.\n",
      "\n",
      "Parameters: regr : string or callable, optional A regression function returning an array of outputs of the linear regression functional basis for Universal Kriging purpose. regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ rho_regr : string or callable, optional A regression function returning an array of outputs of the linear regression functional basis. Defines the regression function for the autoregressive parameter rho. rho_regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ theta : double, array_like or list, optional Value of correlation parameters if they are known; no optimization is run. Default is None, so that optimization is run. if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level theta0 : double, array_like or list, optional Starting point for the maximum likelihood estimation of the best set of parameters. Default is None and meaning use of the default 0.5*np.ones(n_features) if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level thetaL : double, array_like or list, optional Lower bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of the default 1e-5*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level thetaU : double, array_like or list, optional Upper bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of default value 50*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level\n",
      "fit(X, y, initial_range=0.3, tol=1e-06)[source]\n",
      "\n",
      "Implement the Multi-Fidelity co-kriging model fitting method.\n",
      "\n",
      "Parameters: X : list of double array_like elements A list of arrays with the input at which observations were made, from lowest fidelity to highest fidelity. Designs must be nested with X[i] = np.vstack([..., X[i+1]) y : list of double array_like elements A list of arrays with the observations of the scalar output to be predicted, from lowest fidelity to highest fidelity. initial_range : float Initial range for the optimizer. tol : float Optimizer terminates when the tolerance tol is reached.\n",
      "predict(X, eval_MSE=True)[source]\n",
      "\n",
      "Perform the predictions of the kriging model on X.\n",
      "\n",
      "Parameters: X : array_like An array with shape (n_eval, n_features) giving the point(s) at which the prediction(s) should be made. eval_MSE : boolean, optional A boolean specifying whether the Mean Squared Error should be evaluated or not. Default assumes evalMSE is True. array_like An array with shape (n_eval, ) with the Best Linear Unbiased Prediction at X. If all_levels is set to True, an array with shape (n_eval, nlevel) giving the BLUP for all levels. array_like, optional (if eval_MSE is True) An array with shape (n_eval, ) with the Mean Squared Error at X. If all_levels is set to True, an array with shape (n_eval, nlevel) giving the MSE for all levels.\n",
      "rlf(lvl, theta=None)[source]\n",
      "\n",
      "Determine BLUP parameters and evaluate negative reduced likelihood function for theta.\n",
      "\n",
      "Maximizing this function wrt the autocorrelation parameters theta is equivalent to maximizing the likelihood of the assumed joint Gaussian distribution of the observations y evaluated onto the design of experiments X.\n",
      "\n",
      "Parameters: lvl : Integer Level of fidelity theta : array_like, optional An array containing the autocorrelation parameters at which the Gaussian Process model parameters should be determined. Default uses the built-in autocorrelation parameters (ie theta = self.theta). double The value of the negative concentrated reduced likelihood function associated to the given autocorrelation parameters theta.\n",
      "class openmdao.surrogate_models.multifi_cokriging.MultiFiCoKrigingSurrogate(regr='constant', rho_regr='constant', theta=None, theta0=None, thetaL=None, thetaU=None, tolerance=1e-06, initial_range=0.3)[source]\n",
      "\n",
      "OpenMDAO adapter of multi-fidelity recursive cokriging method described in [LeGratiet2013].\n",
      "\n",
      "See MultiFiCoKriging class.\n",
      "\n",
      "Attributes\n",
      "\n",
      " initial_range (float) Initial range for the optimizer. model (MultiFiCoKriging) Contains MultiFiCoKriging surrogate. tolerance (float) Optimizer terminates when the tolerance tol is reached.\n",
      "__init__(regr='constant', rho_regr='constant', theta=None, theta0=None, thetaL=None, thetaU=None, tolerance=1e-06, initial_range=0.3)[source]\n",
      "\n",
      "Initialize all attributes.\n",
      "\n",
      "Parameters: regr : string or callable, optional A regression function returning an array of outputs of the linear regression functional basis for Universal Kriging purpose. regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ rho_regr : string or callable, optional A regression function returning an array of outputs of the linear regression functional basis. Defines the regression function for the autoregressive parameter rho. rho_regr is assumed to be the same for all levels of code. Default assumes a simple constant regression trend. Available built-in regression models are: ‘constant’, ‘linear’ theta : double, array_like or list, optional Value of correlation parameters if they are known; no optimization is run. Default is None, so that optimization is run. if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level theta0 : double, array_like or list, optional Starting point for the maximum likelihood estimation of the best set of parameters. Default is None and meaning use of the default 0.5*np.ones(n_features) if double: value is replicated for all features and all levels. if array_like: an array with shape (n_features, ) for isotropic calculation. It is replicated for all levels. if list: a list of nlevel arrays specifying value for each level thetaL : double, array_like or list, optional Lower bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of the default 1e-5*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level thetaU : double, array_like or list, optional Upper bound on the autocorrelation parameters for maximum likelihood estimation. Default is None meaning use of default value 50*np.ones(n_features). if double: value is replicated for all features and all levels. if array_like: An array with shape matching theta0’s. It is replicated for all levels of code. if list: a list of nlevel arrays specifying value for each level tolerance : float Optimizer terminates when the tolerance tol is reached. initial_range : float Initial range for the optimizer.\n",
      "linearize(x)\n",
      "\n",
      "Calculate the jacobian of the interpolant at the requested point.\n",
      "\n",
      "Parameters: x : array-like Point at which the surrogate Jacobian is evaluated.\n",
      "predict(new_x)[source]\n",
      "\n",
      "Calculate a predicted value of the response based on the current trained model.\n",
      "\n",
      "Parameters: new_x : array_like An array with shape (n_eval, n_features) giving the point(s) at which the prediction(s) should be made. array_like An array with shape (n_eval, ) with the Best Linear Unbiased Prediction at X. If all_levels is set to True, an array with shape (n_eval, nlevel) giving the BLUP for all levels. array_like An array with shape (n_eval, ) with the square root of the Mean Squared Error at X.\n",
      "train(x, y)\n",
      "\n",
      "Calculate a predicted value of the response based on the current trained model.\n",
      "\n",
      "Parameters: x : array-like Point(s) at which the surrogate is evaluated. y : array-like Model responses at given inputs.\n",
      "train_multifi(X, Y)[source]\n",
      "\n",
      "Train the surrogate model with the given set of inputs and outputs.\n",
      "\n",
      "Parameters: X : array_like An array with shape (n_samples_X, n_features) with the input at which observations were made. Y : array_like An array with shape (n_samples_X, n_features) with the observations of the scalar output to be predicted.\n",
      "vectorized_predict(x)\n",
      "\n",
      "Calculate predicted values of the response based on the current trained model.\n",
      "\n",
      "Parameters: x : array-like Vectorized point(s) at which the surrogate is evaluated.\n",
      "openmdao.surrogate_models.multifi_cokriging.constant_regression(x)[source]\n",
      "\n",
      "Zero order polynomial (constant, p = 1) regression model.\n",
      "\n",
      "x –> f(x) = 1\n",
      "\n",
      "Parameters: x : array_like Input data. array_like Constant regression output.\n",
      "openmdao.surrogate_models.multifi_cokriging.l1_cross_distances(X, Y=None)[source]\n",
      "\n",
      "Compute the nonzero componentwise L1 cross-distances between the vectors in X and Y.\n",
      "\n",
      "Parameters: X : array_like An array with shape (n_samples_X, n_features) Y : array_like An array with shape (n_samples_Y, n_features) array with shape (n_samples * (n_samples - 1) / 2, n_features) The array of componentwise L1 cross-distances.\n",
      "openmdao.surrogate_models.multifi_cokriging.linear_regression(x)[source]\n",
      "\n",
      "First order polynomial (linear, p = n+1) regression model.\n",
      "\n",
      "x –> f(x) = [ 1, x_1, ..., x_n ].T\n",
      "\n",
      "Parameters: x : array_like Input data. array_like Linear regression output.\n",
      "openmdao.surrogate_models.multifi_cokriging.squared_exponential_correlation(theta, d)[source]\n",
      "\n",
      "Squared exponential correlation model (Radial Basis Function).\n",
      "\n",
      "(Infinitely differentiable stochastic process, very smooth):\n",
      "\n",
      "                                    n\n",
      "theta, dx --> r(theta, dx) = exp(  sum  - theta_i * (dx_i)^2 )\n",
      "i = 1\n",
      "\n",
      "Parameters: theta : array_like An array with shape 1 (isotropic) or n (anisotropic) giving the autocorrelation parameter(s). d : array_like An array with shape (n_eval, n_features) giving the componentwise distances between locations x and x’ at which the correlation model should be evaluated. r : array_like An array with shape (n_eval, ) containing the values of the autocorrelation model.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_ds_verified[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 18.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.54ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 20.19ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.05ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 20.54ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.16ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 19.15ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 19.78ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 20.28ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 20.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.82ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 19.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 21.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 26.47ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 31.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 33.52ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 34.77ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:01<00:00, 33.69ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 21/21 [05:41<00:00, 16.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Asap7772/open-web-math-processed-v2/commit/a76d9632bbda593918110a46b634b18bedd2af92', commit_message='Upload dataset', commit_description='', oid='a76d9632bbda593918110a46b634b18bedd2af92', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Asap7772/open-web-math-processed-v2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Asap7772/open-web-math-processed-v2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub('Asap7772/open-web-math-processed-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
